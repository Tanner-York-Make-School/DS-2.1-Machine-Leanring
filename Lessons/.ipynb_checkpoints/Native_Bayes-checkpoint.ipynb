{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nagtive Bayes\n",
    "Naive Bayes assumption is that each word is independent of all other words, In reality, this is not true! But through this we'll assume this is true and see where it gets us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "Text classification is the process of attaching labels to bodies of text, e.g., tax document, medical form, etc. based on the content of the text itself.\n",
    "- Think of your spam folder in your email. How does your email provider know that a particular message is spam or “ham” (not spam)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Create spam and ham dictionary\n",
    "1. Create two dictionaries for spam and ham where keys are unique words and values are the frequency of each word\n",
    "    1. Example: if the word \"password\" shows up 4 times in the text, then in the dictionary, the key would be \"password\" and the value would be 4\n",
    "1. Create the dictionaries programatically using for loops\n",
    "1. Use the below text to create your dictionaries:\n",
    "    1. spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "    1. ham_text= ['Send us your review', 'review your password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Dictionary\n",
      "{'send': 3, 'us': 3, 'your': 3, 'password': 2, 'review': 1, 'account': 1}\n",
      "\n",
      "Ham Dictionary\n",
      "{'send': 1, 'us': 1, 'your': 1, 'review': 1, 'password': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "ham_text= ['Send us your review', 'review your password']\n",
    "\n",
    "spam_dict = {}\n",
    "for sentence in spam_text:\n",
    "    for word in sentence.lower().split():\n",
    "        if word in spam_dict:\n",
    "            spam_dict[word] += 1\n",
    "        else:\n",
    "            spam_dict[word] = 1\n",
    "print('Spam Dictionary')\n",
    "print(spam_dict)\n",
    "print('')\n",
    "\n",
    "ham_dict = {}\n",
    "for sentence in ham_text:\n",
    "    for word in sentence.lower().split():\n",
    "        if word in ham_text:\n",
    "            ham_dict[word] += 1\n",
    "        else:\n",
    "            ham_dict[word] = 1\n",
    "print('Ham Dictionary')\n",
    "print(ham_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is the probability that password would be in a spam email?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_password_spam = spam_dict['password'] / sum(spam_dict.values())\n",
    "prob_password_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the probability that password would be in a ham email?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_password_ham = ham_dict['password'] / sum(ham_dict.values())\n",
    "prob_password_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given that the word password is in an email, what are the odds of it being spam?\n",
    "- To figure this out we can use Bayes Rules Law of Total Probability (LOTP)\n",
    "\n",
    "**Bayes' Rule:** $P(spam \\mid password) = (P(password \\mid spam) P(spam))/ P(password)$\n",
    "\n",
    "**LOTP:** $P(password) = P(password \\mid spam) P(spam) + P(password \\mid ham) P(ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Password:\n",
      "0.16923076923076924\n",
      "\n",
      "Probability of spam given password:\n",
      "0.606060606060606\n"
     ]
    }
   ],
   "source": [
    "prob_spam = 4/6\n",
    "prob_ham = 2/6\n",
    "\n",
    "# LOTP\n",
    "prob_password = prob_password_spam * prob_spam + prob_password_ham * prob_ham\n",
    "print(\"Probability of Password:\")\n",
    "print(prob_password)\n",
    "print('')\n",
    "\n",
    "# Bayes Rule\n",
    "prob_spam_password = prob_password_spam * prob_spam / prob_password\n",
    "print(\"Probability of spam given password:\")\n",
    "print(prob_spam_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier (Math)\n",
    "The Bayes Theorem : $P(spam | w_1, w_2, ..., w_n) = {P(w_1, w_2, ..., w_n | spam)P(spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "Naive Bayes assumption is that each word is independent of all other words, In reality, this is not true! But let's try it out for our spam/ham examples:\n",
    "\n",
    "Applying Bayes' Rule, the above relationship becomes simple for both spam and ham with the Naive Bayes assumption:\n",
    "\n",
    "$P(spam | w_1, w_2, ..., w_n) = {P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "$P(ham | w_1, w_2, ..., w_n) = {P(w_1| ham)P(w_2| ham) ... P(w_n| ham)P(ham)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "The denominator $P(w_1, w_2, ..., w_n)$ is independent of spam and ham, so we can remove it to simplify our equations, as we only care about labeling, and proportional relationships:\n",
    "\n",
    "$P(spam | w_1, w_2, ..., w_n) \\propto P(spam | w_1, w_2, ..., w_n) = {P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)}$\n",
    "\n",
    "$P(ham | w_1, w_2, ..., w_n) \\propto P(ham | w_1, w_2, ..., w_n) = {P(w_1| ham)P(w_2| ham) ... P(w_n| ham)P(ham)}$\n",
    "\n",
    "This is easier to express if we can write it as a summation. To do so, we can take the log of both sides of the equation, because the log of a product is the sum of the logs.\n",
    "\n",
    "$logP(spam | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)}$\n",
    "\n",
    "$logP(ham | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "#### Given the above, we can therefore, say that if:\n",
    "\n",
    "${\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)} &gt; {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "#### then that sentence is spam. Otherwise, the sentence is ham!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code for Native Bayes for spam/ham dataset\n",
    "\n",
    "1. Based on the given dataset above, create the following two dictionaries:\n",
    "\n",
    " 1. Ham -> D_ham = {'Jos': 1,'ask':1, 'you':1,... }\n",
    " 1. Spam- > D_spam= {'Did': 1, 'you':3, ... }\n",
    "\n",
    "    1. Each dictionary representes all words for the spam and ham emails and their frequency (as the value of dictionaries)\n",
    "\n",
    "2. For any new given sentences, having $w_1$, $w_2$, ... $w_n$ words, assuming the sentence is ham, calculate the following:\n",
    "\n",
    "     1. $P(w_1| ham)$, $P(w_2| ham)$, ..., $P(w_n| ham)$ $log(P(w_1| ham))$, $log(P(w_2| ham))$, ..., $log(P(w_n| ham))$\n",
    "\n",
    "        1. then add them all together to create one value\n",
    "\n",
    "3. Calculate what percentage of labels are ham -> $P(ham)$ -> then take the log -> $log(P(ham))$\n",
    "\n",
    "4. Add the value from step (2) and (3)\n",
    "\n",
    "5. Do Steps (2) - (4) again, but assume the given new sentence is spam\n",
    "\n",
    "6. Compare the two values. The greater value indicates which label (class) the sentence should be given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply the naive Bayes to spam/ham email dataset:\n",
    "In groups of 3, complete the following activity\n",
    "\n",
    "1. Please read this article, starting at the Naive Bayes Assumption section: https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/\n",
    "2. We will use the Spam Dataset\n",
    "3. In the article, for the codeblock of the fit method, which line(s) of the method calculates the probabilty of ham and spam?\n",
    "4. For the same fit method, which line(s) of the method calculates the spam and ham dictionaries?\n",
    "5. In the article, for the codeblock of the predict method, which line(s) compares the scores of ham or spam based on log probabilities?\n",
    "\n",
    "We will discuss as a class after workinging in groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Find the Naive Bayes core parts in the SpamDetector Class\n",
    "***In groups of 3, complete the following activity***\n",
    "\n",
    "Assume we have written the SpamDetector class from the article. Train this model from the given Spam Dataset, and use it to make a prediction!\n",
    "\n",
    "Use the starter code below, and then fill in the TODOs in the main.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- you will need to use train_test_split from sklearn to obtain your training and test (prediction) data\n",
    "- You will need to instantiate your SpamDetector, fit the training data to it, predict using the test values, and then measure the accuracy\n",
    "- To calculate accuracy: add up all the correct predictions divided by the total number of predictions\n",
    "- Use the following code to get your data ready for transforming/manipulating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "\n",
    "    # clean up our string by removing punctuation\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isn’t already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "\n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hame/Spam Model Accuracy: 0.9612347451543432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('../Data/Spam.csv', encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "X, y = texts, tags\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "spam_detector = SpamDetector()\n",
    "spam_detector.fit(X_train, y_train)\n",
    "\n",
    "y_pred = spam_detector.predict(X_test)\n",
    "true = y_test.values\n",
    "\n",
    "print('Hame/Spam Model Accuracy:', accuracy_score(y_pred, true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: use sklearn CountVectorizer and MultinomialNB to spam email dataset\n",
    "As we've seen with previous topics, sklearn has a lot of built in functionality that can save us from writing the code from scratch. We are going to solve the same problem in the previous activity, but using sklearn!\n",
    "\n",
    "For example, the SpamDectector class in the previous activity is an example of a **Multinomial Naive Bayes (MNB)** model. An MNB lets us know that each conditional probability we're looking at (i.e. $P(spam | w_1, w_2, ..., w_n)$) is a multinomial (several terms, polynomial) distribution, rather than another type distribution.\n",
    "\n",
    "In groups of 3, complete the activity by using the provided starter code and following the steps below:\n",
    "\n",
    "1 - Split the dataset\n",
    "\n",
    "2 - Vectorize the dataset : vect = CountVectorizer()\n",
    "\n",
    "3 - Transform training data into a document-term matrix (BoW): X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "4 - Build and evaluate the model\n",
    "\n",
    "Hints:\n",
    "\n",
    "Remember how you prepared/cleaned/labeled the dataset, created texts and tags, and split the data innto train vs test from the previous activity. You'll need to do so again here\n",
    "Review the CountVectorizer documentation to see how you can transform text into numerical vectors\n",
    "Need more help? Check out this MNB Vectorization article and see what you can use from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9870782483847811"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare the dataset\n",
    "data = pd.read_csv('../Data/Spam.csv', encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "\n",
    "# TODO: create texts and tags\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "X, y = texts, tags\n",
    "\n",
    "# Split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# TODO: transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
